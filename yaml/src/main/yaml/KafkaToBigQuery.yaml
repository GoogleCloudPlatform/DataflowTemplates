template:

  name: "Kafka_to_BigQuery_Yaml"
  category: "STREAMING"
  type: "YAML"
  display_name: "Kafka to BigQuery (YAML)"
  description: >
    The Apache Kafka to BigQuery template is a streaming pipeline which ingests 
    text data from Apache Kafka, executes a user-defined function (UDF), and 
    outputs the resulting records to BigQuery. Any errors which occur in the 
    transformation of the data, execution of the UDF, or inserting into the 
    output table are inserted into a separate errors table in BigQuery. 
    If the errors table does not exist prior to execution, then it is created.
  flex_container_name: "kafka-to-bigquery-yaml"
  yamlTemplateFile: "KafkaToBigQuery.yaml"
  filesToCopy: >
    {"main.py", "requirements.txt"}
  documentation: >
    https://cloud.google.com/dataflow/docs/guides/templates/provided/kafka-to-bigquery
  contactInformation: "https://cloud.google.com/support"
  requirements: {
    "The output BigQuery table must exist.",
    "The Apache Kafka broker server must be running and be reachable from the Dataflow worker machines.",
    "The Apache Kafka topics must exist and the messages must be encoded in a valid JSON format."
  }
  streaming: true
  hidden: false

  options_file:
    - "kafka_options"
    - "bigquery_options"



  parameters:
    - kafka_read_options
    - bigquery_common_options
    - bigquery_write_options

    - name: "outputDeadletterTable"
      description: "The dead-letter table name to output failed messages to BigQuery"
      help: >
        BigQuery table for failed messages. Messages failed to reach the output 
        table for different reasons (e.g., mismatched schema, malformed json) 
        are written to this table. If it doesn't exist, it will be created 
        during pipeline execution. If not specified, 
        'outputTableSpec_error_records' is used instead. The dead-letter table
        name to output failed messages to BigQuery.
      example: "your-project-id:your-dataset.your-table-name"
      required: true
      type: text
      order: 1

pipeline:
  transforms:
    - type: ReadFromKafka
      config:
        {% if schema %}
        schema: |
          {{ schema}}
        {% endif %}
        format: {{ format  | default('JSON') }}
        topic: {{ topic }}
        bootstrap_servers: {{ bootstrapServers }}
        auto_offset_reset_config: 'earliest'
        error_handling:
          output: errors
    - type: WriteToBigQuery
      name: WriteGoodMessages
      input: ReadFromKafka
      config:
        table: {{ table }}
        create_disposition: {{ createDisposition | default('CREATE_IF_NEEDED') }}
        write_disposition: {{ writeDisposition | default('WRITE_APPEND') }}
        num_streams: {{ numStreams | default(1) }}
        error_handling:
          output: errors
    - type: WriteToBigQuery
      name: WriteBadReadMessages
      input: ReadFromKafka.errors
      config:
        table: {{ outputDeadletterTable }}
        num_streams: {{ numStreams | default(1) }}
    - type: WriteToBigQuery
      name: WriteBadWriteMessages
      input: WriteGoodMessages.errors
      config:
        table: {{ outputDeadletterTable }}
        num_streams: {{ numStreams | default(1) }}

options:
  streaming: true
